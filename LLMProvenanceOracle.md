# Large Language Model Oracle With Data Provenance
This document describes an oracle for prediction markets that retrieves data from external websites, gathers cryptographic evidence of the dataâ€™s provenance, and resolves the market using a deterministic large language model.

## The Protocol 
A prediction market is created with the following parameters fixed at deployment time:
 - A deterministic large language model
 - A predefined set of external data sources and their public keys
 - A deterministic toolchain (eg, curl, parsing)
 - market question, description, resolution time, outcome options

All participants agree upfront on the complete execution specification.

## 1 Resolution Phase
After the market resolves:

1. A resolver runs the Large Language Model inside the agreed deterministic environment.
2. Whenever the Large Language Model performs a web request:
 - The raw response is recorded.
 - A cryptographic proof is obtained that the response originated from the claimed website along with the timestamp (provided by the website)
3. All fetched data is stored and recorded

The resolver submits onchain:
 - Final resolution result.
 - All external data used.
 - Proofs that each piece of data originated from the declared source.

## 2 Verification Phase
After all the data is onchain, anyone can verify the trace by either re-executing everything (using given website data in place of actually doing the web requests) using optimistic protocol or by verifying ZK proof generated by the resolver. The verificator also checks that the website data signatures are correct and that the data actually came from the respective websites. This proof must also include timestamps from the websites to be sure that the data was not old (prior maket resolution date).

If multiple resolutions are submitted, the first valid resolution is considered as final.

## Technological challenges
There's couple technological challenges with the design:
- A lot data is generated, all this data need to be accessible for anyone. For this eg, [IPFS](https://ipfs.tech/) or Ethereum Blob storage might be used. Other approach is to try to limit the payload size coming from webpages
- The websites need to sign the data they provide. Currently it seems like no websites are doing this, even thought there could be protocols that achieve this (eg, OAuth and [JWT](https://www.jwt.io/)). [TLS-notary](https://tlsnotary.org/) or [DECO](https://www.deco.works/) sound like reasonable options, but they require interactive multiparty computation.
- The Large Language Model needs to be deterministic
- We need to be able to prove Large Language Model execution. Here [Cartesi](https://cartesi.io/) could work. Proving execution with ZK could be nice, but technically too hard.
- The model can hallusinate. This probability should be minimized
- The websites can trick the Large Language Model to resolve wrong with prompt injections. This can be alleviated by running the Large Language Model in separate context for each website and each website vote the market to resolve in certain way, then the votes are tallied and majority vote wins.
